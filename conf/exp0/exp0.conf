name = exp0
debug = False
training_data = data/amazon.json.gz
output_dir = conf/exp0/output
min_test_features = 0.01
min_train_features = 1.0

[feature_extraction]
    min_df = 1
    k = 30
    lemmatize = True
    use_pos = True
    coarse_pos = True
    normalise_entities = False
    use_tfidf = False
    record_stats = False
    sim_compressor = thesisgenerator.utils.misc.one
    decode_token_handler = thesisgenerator.plugins.bov_feature_handlers.SignifiedOnlyFeatureHandler
    remove_features_with_NER = True
    random_neighbour_thesaurus = False
    [[train_time_opts]]
        extract_unigram_features = J, N    # V
        extract_phrase_features = AN, NN    # VO, SVO
    [[decode_time_opts]]
        extract_unigram_features = ,    # J,N,V or a single comma to indicate nothing
        extract_phrase_features = AN, NN    # VO, SVO

[tokenizer]
    lowercase = True
    keep_only_IT = False
    remove_stopwords = True
    remove_short_words = False
    remove_long_words = True    # probably noise anyway.

[crossvalidation]
    k = 5
    random_state = 0

[feature_selection]
    must_be_in_thesaurus = True    # remove document features without a distributional representation
    min_log_odds_score = 0.0    # do not remove features that are associated with both classes
    k = 99999999999    # do not use chi2 

[classifiers]
    [[sklearn.naive_bayes.MultinomialNB]]
        run = True
        alpha = 0.001
    
    [[sklearn.linear_model.LogisticRegression]]
        run = True
        C = 1e-05

[vector_sources]
    sim_threshold = -9999999999.0    # we should be loading vectors here, so this is a reasonable threshold.
    neighbours_file = data/AN_NN_gigaw-100_Add.events.filtered.strings,
    max_neighbours = 2000000000
    allow_lexical_overlap = False
    use_shelf = False
    entries_of = ""
    neighbour_strategy = linear
    noise = 0.2
